{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myang4/miniconda3/envs/vllm3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from datasets import Dataset, load_dataset\n",
    "from random import randint, seed, choice\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divisors_below(n, limit):\n",
    "    if n < 0:\n",
    "        n = -n\n",
    "    if n == 0:\n",
    "        return list(range(1, limit))\n",
    "\n",
    "    divisors = set()\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            if i < limit:\n",
    "                divisors.add(i)\n",
    "            if (n // i) < limit:\n",
    "                divisors.add(n // i)\n",
    "    return sorted(divisors)\n",
    "\n",
    "def gen_dataset(\n",
    "    num_samples: int,\n",
    "    num_operands: int = 6,\n",
    "    max_operand: int = 100,\n",
    "    max_target: int = 1000,\n",
    "    operations: List[str] = ['+', '-', '*', '/'],\n",
    "    seed_value: int = 42,\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Generate dataset for countdown task.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of samples to generate\n",
    "        num_operands: Number of numbers provided in each sample\n",
    "        max_target: Maximum value for target number\n",
    "        min_number: Minimum value for provided numbers\n",
    "        max_number: Maximum value for provided numbers\n",
    "        operations: List of allowed operations\n",
    "        seed_value: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples containing (target, numbers, solution)\n",
    "    \"\"\"\n",
    "    seed(seed_value)\n",
    "    samples = set()\n",
    "    \n",
    "    while len(samples) < num_samples:\n",
    "        # Generate random target\n",
    "        target = randint(1, max_target)\n",
    "        original_target = target\n",
    "\n",
    "        numbers = []\n",
    "        for _ in range(num_operands - 1):\n",
    "            op = choice(operations)\n",
    "            \n",
    "            if op == '+':\n",
    "                num = randint(1, max_operand)\n",
    "                target += num\n",
    "            \n",
    "            elif op == '-':\n",
    "                num = randint(1, max_operand)\n",
    "                target -= num\n",
    "\n",
    "            elif op == '*':\n",
    "                num = randint(1, max_operand)\n",
    "                target *= num\n",
    "            \n",
    "            elif op == '/':\n",
    "                divisors = get_divisors_below(n=target, limit=max_operand)\n",
    "                num = choice(divisors)\n",
    "                target //= num\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Invalid operation: {op}\")\n",
    "        \n",
    "            assert 1 <= num <= max_operand\n",
    "            numbers.append(num)\n",
    "        \n",
    "        if 1 <= target <= max_operand:\n",
    "            numbers.append(target)\n",
    "            samples.add(tuple([original_target, tuple(numbers)]))\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_countdown(candidates, target):\n",
    "    if len(candidates) == 1:\n",
    "        return abs(candidates[0] - target) < 0.001, []\n",
    "    \n",
    "    ans = False\n",
    "    for i in range(len(candidates)):\n",
    "        for j in range(i+1, len(candidates)):\n",
    "            ops = [\n",
    "                (f\"{candidates[i]} + {candidates[j]}\", candidates[i] + candidates[j]), \n",
    "                (f\"{candidates[i]} - {candidates[j]}\", candidates[i] - candidates[j]), \n",
    "                (f\"{candidates[j]} - {candidates[i]}\", candidates[j] - candidates[i]), \n",
    "                (f\"{candidates[i]} * {candidates[j]}\", candidates[i] * candidates[j])\n",
    "            ]\n",
    "            \n",
    "            if candidates[i] != 0:\n",
    "                ops.append((f\"{candidates[j]} / {candidates[i]}\", candidates[j] / candidates[i]))\n",
    "            if candidates[j] != 0:\n",
    "                ops.append((f\"{candidates[i]} / {candidates[j]}\", candidates[i] / candidates[j]))\n",
    "            \n",
    "            new_candidates = [candidates[k] for k in range(len(candidates)) if k != i and k != j]\n",
    "\n",
    "            for op_name, op in ops:\n",
    "                ans, op_lst = solve_countdown(new_candidates+[op], target)\n",
    "                if ans:\n",
    "                    return ans, [(candidates, op_name, new_candidates+[op])] + op_lst\n",
    "\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_to_dataset = defaultdict(set)\n",
    "for difficulty in range(2, 8):\n",
    "    difficulty_to_dataset[difficulty] = list(gen_dataset(11000, num_operands=difficulty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(split):\n",
    "    if split == 'train':\n",
    "        for difficulty in range(2, 8):\n",
    "            for target, numbers in difficulty_to_dataset[difficulty][:10000]:\n",
    "                yield {'target': target, 'nums': numbers}\n",
    "    elif split == 'test':\n",
    "        for difficulty in range(2, 8):\n",
    "            for target, numbers in difficulty_to_dataset[difficulty][10000:]:\n",
    "                yield {'target': target, 'nums': numbers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.from_generator(gen, gen_kwargs={\"split\": 'train'})\n",
    "test = Dataset.from_generator(gen, gen_kwargs={\"split\": 'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 60/60 [00:00<00:00, 2999.79ba/s]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-67cf4c92-67a2d64164e27a3b1f95d75b;985d6fce-91d7-418d-8134-51f67b6ec3d8)\n\n403 Forbidden: Private repository storage limit reached, please upgrade your plan to increase your private storage limit.\nCannot access content at: https://huggingface.co/datasets/CMU-AIRe/countdown.git/info/lfs/objects/batch.\nMake sure your token has the correct permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/datasets/CMU-AIRe/countdown.git/info/lfs/objects/batch",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m hub_dataset_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCMU-AIRe/countdown\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub_dataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m test.push_to_hub(\n\u001b[32m     11\u001b[39m     hub_dataset_name,\n\u001b[32m     12\u001b[39m     revision=\u001b[33m'\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     split=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m     private=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/datasets/arrow_dataset.py:5460\u001b[39m, in \u001b[36mDataset.push_to_hub\u001b[39m\u001b[34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_dir:\n\u001b[32m   5458\u001b[39m     data_dir = config_name \u001b[38;5;28;01mif\u001b[39;00m config_name != \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5460\u001b[39m additions, uploaded_size, dataset_nbytes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_push_parquet_shards_to_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5462\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5463\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5469\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_external_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5470\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5472\u001b[39m \u001b[38;5;66;03m# Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)\u001b[39;00m\n\u001b[32m   5473\u001b[39m \u001b[38;5;66;03m# and delete old split shards (if they exist)\u001b[39;00m\n\u001b[32m   5474\u001b[39m repo_with_dataset_card, repo_with_dataset_infos = \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/datasets/arrow_dataset.py:5301\u001b[39m, in \u001b[36mDataset._push_parquet_shards_to_hub\u001b[39m\u001b[34m(self, repo_id, data_dir, split, token, revision, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[39m\n\u001b[32m   5299\u001b[39m     uploaded_size += buffer.tell()\n\u001b[32m   5300\u001b[39m     shard_addition = CommitOperationAdd(path_in_repo=shard_path_in_repo, path_or_fileobj=buffer)\n\u001b[32m-> \u001b[39m\u001b[32m5301\u001b[39m     \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreupload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5303\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshard_addition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5308\u001b[39m     additions.append(shard_addition)\n\u001b[32m   5310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m additions, uploaded_size, dataset_nbytes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4249\u001b[39m, in \u001b[36mHfApi.preupload_lfs_files\u001b[39m\u001b[34m(self, repo_id, additions, token, repo_type, revision, create_pr, num_threads, free_memory, gitignore_content)\u001b[39m\n\u001b[32m   4243\u001b[39m     logger.info(\n\u001b[32m   4244\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSkipped upload for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions)\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(new_lfs_additions_to_upload)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m LFS file(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(ignored by gitignore file).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4246\u001b[39m     )\n\u001b[32m   4248\u001b[39m \u001b[38;5;66;03m# Upload new LFS files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4249\u001b[39m \u001b[43m_upload_lfs_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4250\u001b[39m \u001b[43m    \u001b[49m\u001b[43madditions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_lfs_additions_to_upload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4256\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If `create_pr`, we don't want to check user permission on the revision as users with read permission\u001b[39;49;00m\n\u001b[32m   4257\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# should still be able to create PRs even if they don't have write permission on the target branch of the\u001b[39;49;00m\n\u001b[32m   4258\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# PR (i.e. `revision`).\u001b[39;49;00m\n\u001b[32m   4259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4260\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m addition \u001b[38;5;129;01min\u001b[39;00m new_lfs_additions_to_upload:\n\u001b[32m   4262\u001b[39m     addition._is_uploaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/_commit_api.py:389\u001b[39m, in \u001b[36m_upload_lfs_files\u001b[39m\u001b[34m(additions, repo_type, repo_id, headers, endpoint, num_threads, revision)\u001b[39m\n\u001b[32m    387\u001b[39m batch_actions: List[Dict] = []\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunk_iterable(additions, chunk_size=\u001b[32m256\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     batch_actions_chunk, batch_errors_chunk = \u001b[43mpost_lfs_batch_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupload_infos\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# already passed in 'headers'\u001b[39;49;00m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# If at least 1 error, we do not retrieve information for other chunks\u001b[39;00m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch_errors_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/lfs.py:168\u001b[39m, in \u001b[36mpost_lfs_batch_info\u001b[39m\u001b[34m(upload_infos, token, repo_type, repo_id, revision, endpoint, headers)\u001b[39m\n\u001b[32m    162\u001b[39m headers = {\n\u001b[32m    163\u001b[39m     **LFS_HEADERS,\n\u001b[32m    164\u001b[39m     **build_hf_headers(token=token),\n\u001b[32m    165\u001b[39m     **(headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    166\u001b[39m }\n\u001b[32m    167\u001b[39m resp = get_session().post(batch_url, headers=headers, json=payload)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m batch_info = resp.json()\n\u001b[32m    171\u001b[39m objects = batch_info.get(\u001b[33m\"\u001b[39m\u001b[33mobjects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm3/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:468\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    463\u001b[39m     message = (\n\u001b[32m    464\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    471\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-67cf4c92-67a2d64164e27a3b1f95d75b;985d6fce-91d7-418d-8134-51f67b6ec3d8)\n\n403 Forbidden: Private repository storage limit reached, please upgrade your plan to increase your private storage limit.\nCannot access content at: https://huggingface.co/datasets/CMU-AIRe/countdown.git/info/lfs/objects/batch.\nMake sure your token has the correct permissions."
     ]
    }
   ],
   "source": [
    "hub_dataset_name = f\"CMU-AIRe/countdown\"\n",
    "\n",
    "train.push_to_hub(\n",
    "    hub_dataset_name,\n",
    "    revision='main',\n",
    "    split='train',\n",
    "    private=True,\n",
    ")\n",
    "\n",
    "test.push_to_hub(\n",
    "    hub_dataset_name,\n",
    "    revision='main',\n",
    "    split='test',\n",
    "    private=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
